{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPd61sEG6covTrea8u6ecPG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"382ad59b03994c70a67a2ae2f287ab6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2fbad95b314a4688ae760ead39b8e6bb","IPY_MODEL_4b089aab0f664430a905ebc84564eabf","IPY_MODEL_7b5ac9e0ef0c494fbc8d157740ea666f"],"layout":"IPY_MODEL_92503f02313c4a9a96681352ed111087"}},"2fbad95b314a4688ae760ead39b8e6bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_743e561bef204a11ad4b8e68cffc6a49","placeholder":"​","style":"IPY_MODEL_1ce0b25e26fe4969b3a58f8c77b4f49e","value":"modules.json: 100%"}},"4b089aab0f664430a905ebc84564eabf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84b68c8930ee4e4bae708e732320600d","max":349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_582e99fc47a8426892cd22d2eb0710dd","value":349}},"7b5ac9e0ef0c494fbc8d157740ea666f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e74222553024b369d825dc0bd521f89","placeholder":"​","style":"IPY_MODEL_fc81161314d8404b8a24100065f08303","value":" 349/349 [00:00&lt;00:00, 28.0kB/s]"}},"92503f02313c4a9a96681352ed111087":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"743e561bef204a11ad4b8e68cffc6a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce0b25e26fe4969b3a58f8c77b4f49e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84b68c8930ee4e4bae708e732320600d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"582e99fc47a8426892cd22d2eb0710dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e74222553024b369d825dc0bd521f89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc81161314d8404b8a24100065f08303":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3b15795e34747ab95e35d511db927b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19b721a83a7b48fba2237e50c5fb1515","IPY_MODEL_ee1240838b774875a794970b5d47513b","IPY_MODEL_b8bdf377a77d41938b7de3cbce3bf97c"],"layout":"IPY_MODEL_b17ad4f673c04380bf2b8b8a0abbc8e7"}},"19b721a83a7b48fba2237e50c5fb1515":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea966cf1608f4a1faee0063488898819","placeholder":"​","style":"IPY_MODEL_6716188e90104a2b8369d0589f09207b","value":"config_sentence_transformers.json: 100%"}},"ee1240838b774875a794970b5d47513b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab87af2601544f9a9482906654ff8059","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_578704c4e95e4f6c913f03b869f3b347","value":116}},"b8bdf377a77d41938b7de3cbce3bf97c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f580e9fcb671494fa20ca3962ec2ebe9","placeholder":"​","style":"IPY_MODEL_112cad1abfc347648c55edf4dc4f13d3","value":" 116/116 [00:00&lt;00:00, 8.96kB/s]"}},"b17ad4f673c04380bf2b8b8a0abbc8e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea966cf1608f4a1faee0063488898819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6716188e90104a2b8369d0589f09207b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab87af2601544f9a9482906654ff8059":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"578704c4e95e4f6c913f03b869f3b347":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f580e9fcb671494fa20ca3962ec2ebe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"112cad1abfc347648c55edf4dc4f13d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0fa18ee4151496789660176a71daabb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_671157d41bb449b68661b8cfaf0aff76","IPY_MODEL_cce5ef96020a4ae6b04cc7b9bcff633d","IPY_MODEL_ff85becf6fdc40f1bd9cf5cc03593056"],"layout":"IPY_MODEL_a1ec9513622a4863a221e6dee10afaf8"}},"671157d41bb449b68661b8cfaf0aff76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80326ab256274b8ab7e6e26b3785d97e","placeholder":"​","style":"IPY_MODEL_656d169134a34419a64b871dd257316b","value":"README.md: 100%"}},"cce5ef96020a4ae6b04cc7b9bcff633d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7408c18d0d5413d986529f271abf48d","max":10621,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a46b5ac422f3476db8cacbace4d6e123","value":10621}},"ff85becf6fdc40f1bd9cf5cc03593056":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_106db31c469b40189c97e9052f618ba2","placeholder":"​","style":"IPY_MODEL_35c12ae63dff4f1c83765aebcb4c31b1","value":" 10.6k/10.6k [00:00&lt;00:00, 625kB/s]"}},"a1ec9513622a4863a221e6dee10afaf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80326ab256274b8ab7e6e26b3785d97e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"656d169134a34419a64b871dd257316b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7408c18d0d5413d986529f271abf48d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46b5ac422f3476db8cacbace4d6e123":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"106db31c469b40189c97e9052f618ba2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35c12ae63dff4f1c83765aebcb4c31b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e6edc6c21764a73acaa4508acfeb492":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91e48ac0fb5b45bb91af06f0beea6023","IPY_MODEL_7a4b4358260b4c37882772e389f5ca57","IPY_MODEL_f23a1d5f6c2c4cb78ea003a45d952a9e"],"layout":"IPY_MODEL_37340df537c7473fb98e3afad2e86cc5"}},"91e48ac0fb5b45bb91af06f0beea6023":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4957cc48b7e743cab3fe8faac0529c60","placeholder":"​","style":"IPY_MODEL_c83a219426764bc4913e016a333ba2be","value":"sentence_bert_config.json: 100%"}},"7a4b4358260b4c37882772e389f5ca57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfcc1b43be5944068825e46533369b88","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d8fbe4a03404727a1960c673af2fcdc","value":53}},"f23a1d5f6c2c4cb78ea003a45d952a9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8bd0398573d4905be7c90a77c36071a","placeholder":"​","style":"IPY_MODEL_2a8951ab586d4fd2b64a80257231cbe4","value":" 53.0/53.0 [00:00&lt;00:00, 4.76kB/s]"}},"37340df537c7473fb98e3afad2e86cc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4957cc48b7e743cab3fe8faac0529c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c83a219426764bc4913e016a333ba2be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfcc1b43be5944068825e46533369b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d8fbe4a03404727a1960c673af2fcdc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8bd0398573d4905be7c90a77c36071a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a8951ab586d4fd2b64a80257231cbe4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0234946a8ec945ac83a4993ef369ef1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33c3aa67fcf84fc497b88204375f65fb","IPY_MODEL_f052f450d5a7498ca2988e075ea77607","IPY_MODEL_0815906440cc4b44a29bb1da4400e46c"],"layout":"IPY_MODEL_7ba4c15b23cb4bfca7ac4a7a4c4e8720"}},"33c3aa67fcf84fc497b88204375f65fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3143dafe734e4f6387eccb34729058f4","placeholder":"​","style":"IPY_MODEL_698fee7f2fa945e195d73b3fb26e2c50","value":"config.json: 100%"}},"f052f450d5a7498ca2988e075ea77607":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fa2d99070ce431f927dd089697218f0","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c397a43dac1492fb37d2859040cb962","value":571}},"0815906440cc4b44a29bb1da4400e46c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11e1dd68d7424660962b5418561a182c","placeholder":"​","style":"IPY_MODEL_447e2b52e7294966b2338f50184ffe1d","value":" 571/571 [00:00&lt;00:00, 49.0kB/s]"}},"7ba4c15b23cb4bfca7ac4a7a4c4e8720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3143dafe734e4f6387eccb34729058f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"698fee7f2fa945e195d73b3fb26e2c50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fa2d99070ce431f927dd089697218f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c397a43dac1492fb37d2859040cb962":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11e1dd68d7424660962b5418561a182c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447e2b52e7294966b2338f50184ffe1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b75c8e9c7354228aed1cc2e493a35cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ed83ce3d6dc4547af5094edd81018eb","IPY_MODEL_919ebc9172584453b41badc9246466f3","IPY_MODEL_dca4dd78c43d41e79d2dda5d0028f48c"],"layout":"IPY_MODEL_b96f10256846400296feeef200a1f7e0"}},"5ed83ce3d6dc4547af5094edd81018eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b934a5cbd02843b0b933889e86080c61","placeholder":"​","style":"IPY_MODEL_bfcd15baa28347d4ac5526a0fd46b7e4","value":"model.safetensors: 100%"}},"919ebc9172584453b41badc9246466f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dd3773e61594ab5a53b6d34e9d43535","max":437971872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d639e8a1e0845ca97a4d3811251b93f","value":437971872}},"dca4dd78c43d41e79d2dda5d0028f48c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe5b6852419040c38743e9b4b388bf6f","placeholder":"​","style":"IPY_MODEL_ab551537a0134954952321cdd581d927","value":" 438M/438M [00:02&lt;00:00, 214MB/s]"}},"b96f10256846400296feeef200a1f7e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b934a5cbd02843b0b933889e86080c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfcd15baa28347d4ac5526a0fd46b7e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dd3773e61594ab5a53b6d34e9d43535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d639e8a1e0845ca97a4d3811251b93f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe5b6852419040c38743e9b4b388bf6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab551537a0134954952321cdd581d927":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8473d3a033744ca981c2d9ea6eaa95ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_decbc39910af4b9487c506388269837d","IPY_MODEL_3fe4baa29f28417a9e119cc2f1f96495","IPY_MODEL_25437d0d3f0047468f70f0279d7acc1c"],"layout":"IPY_MODEL_94814c38c1ec49c48c393d33d7f3aa91"}},"decbc39910af4b9487c506388269837d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6650384426b41e9909d5758721ac88a","placeholder":"​","style":"IPY_MODEL_8b9a93607e594f329b9ffac086371085","value":"tokenizer_config.json: 100%"}},"3fe4baa29f28417a9e119cc2f1f96495":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4079dc205314fbd930a9cdba8e620f3","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2401e19646df4a8988f23e9ef126cebe","value":363}},"25437d0d3f0047468f70f0279d7acc1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e83f6a0098444b5b94850edab5cc290","placeholder":"​","style":"IPY_MODEL_04463fadfe1f4298b198634caf5b24b3","value":" 363/363 [00:00&lt;00:00, 28.8kB/s]"}},"94814c38c1ec49c48c393d33d7f3aa91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6650384426b41e9909d5758721ac88a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b9a93607e594f329b9ffac086371085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4079dc205314fbd930a9cdba8e620f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2401e19646df4a8988f23e9ef126cebe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e83f6a0098444b5b94850edab5cc290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04463fadfe1f4298b198634caf5b24b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36eec1703124414fbd179b29f67c61b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6917fb66d9c4b0c907d08071c0f698c","IPY_MODEL_692a6aafb51d4bb491ea8c1f2154640c","IPY_MODEL_f6b96c6ca56446aaaf2d6c9d20870021"],"layout":"IPY_MODEL_e6b913eb42fb47a4a2b475a6e978e9bc"}},"b6917fb66d9c4b0c907d08071c0f698c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6777c6da2d294de7814c2e57f13e87f2","placeholder":"​","style":"IPY_MODEL_508b6398949946a5a8ae9d68b2d6f3b9","value":"vocab.txt: 100%"}},"692a6aafb51d4bb491ea8c1f2154640c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3999e126b65748b1a601410ff942adb0","max":231536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5876d496013f4ebfb9ff98b1a3bc16b2","value":231536}},"f6b96c6ca56446aaaf2d6c9d20870021":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d81281b9bc434d3bb0d187fd8b658da8","placeholder":"​","style":"IPY_MODEL_dec0c24b5d4b4e3f859ae2c184ebcb37","value":" 232k/232k [00:00&lt;00:00, 532kB/s]"}},"e6b913eb42fb47a4a2b475a6e978e9bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6777c6da2d294de7814c2e57f13e87f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"508b6398949946a5a8ae9d68b2d6f3b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3999e126b65748b1a601410ff942adb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5876d496013f4ebfb9ff98b1a3bc16b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d81281b9bc434d3bb0d187fd8b658da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec0c24b5d4b4e3f859ae2c184ebcb37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d211dad6624d410c9159419f71246de9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ac34ef3f13648ba9235aa2f364ecd7e","IPY_MODEL_cb5a4322fadc45e29386dfa76f773073","IPY_MODEL_4a607deeed254fd1ae925479023ff243"],"layout":"IPY_MODEL_efcd7134bac741889f2319053d485dff"}},"7ac34ef3f13648ba9235aa2f364ecd7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a6d04239c1b4a818777a19089ecf4ce","placeholder":"​","style":"IPY_MODEL_80e89d31ea2648ac952dfd38ee9c3219","value":"tokenizer.json: 100%"}},"cb5a4322fadc45e29386dfa76f773073":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31ac8d2876b34a668ed74642e692e212","max":466021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e147f2aabba84516ad21449ba492f8ca","value":466021}},"4a607deeed254fd1ae925479023ff243":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dde854b439d4e15ac6b799ad809b868","placeholder":"​","style":"IPY_MODEL_23b603ba04694577a96f150820e620f0","value":" 466k/466k [00:00&lt;00:00, 941kB/s]"}},"efcd7134bac741889f2319053d485dff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a6d04239c1b4a818777a19089ecf4ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80e89d31ea2648ac952dfd38ee9c3219":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31ac8d2876b34a668ed74642e692e212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e147f2aabba84516ad21449ba492f8ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1dde854b439d4e15ac6b799ad809b868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b603ba04694577a96f150820e620f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b42db315abc34d22a5f6b5719330093d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_722c03dfca0f4616a81166a74a244912","IPY_MODEL_92605a1f24484f2b80664cb273779e14","IPY_MODEL_c231eecf50ab4740ba553f357a719a95"],"layout":"IPY_MODEL_b60fe42230294f179e429696ba370615"}},"722c03dfca0f4616a81166a74a244912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91e3887e185f458da4cc87dd67c41fe6","placeholder":"​","style":"IPY_MODEL_814e89ec5623441fa2f99be78e2d14cf","value":"special_tokens_map.json: 100%"}},"92605a1f24484f2b80664cb273779e14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f227d6f720c34b09a403627b89b56514","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b7df8c185af41f29890f98acd81ed91","value":239}},"c231eecf50ab4740ba553f357a719a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ffee34a0e2a4be4aca3d08a22132a66","placeholder":"​","style":"IPY_MODEL_d686612782e0494cb71ec58015f4cc61","value":" 239/239 [00:00&lt;00:00, 20.5kB/s]"}},"b60fe42230294f179e429696ba370615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91e3887e185f458da4cc87dd67c41fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"814e89ec5623441fa2f99be78e2d14cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f227d6f720c34b09a403627b89b56514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b7df8c185af41f29890f98acd81ed91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ffee34a0e2a4be4aca3d08a22132a66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d686612782e0494cb71ec58015f4cc61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd06b5befe07446aa981c750976b6310":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04e3e1eadbcb4788b35677245c9d2901","IPY_MODEL_2222386db32e421e9ec481d09e3d9bf0","IPY_MODEL_b2a195c0ff64448db15d8df7d1f1a7d6"],"layout":"IPY_MODEL_9b9d39e6a7ab4df49949015bb1fe29e9"}},"04e3e1eadbcb4788b35677245c9d2901":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2025da293b345f1b1f2d0edea9dc70a","placeholder":"​","style":"IPY_MODEL_b8bd25946038461cbb779b09a8c030f4","value":"1_Pooling/config.json: 100%"}},"2222386db32e421e9ec481d09e3d9bf0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a1567e9c414a91b2c55550effb4598","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_354b423edbc945639b5095fbb6caa506","value":190}},"b2a195c0ff64448db15d8df7d1f1a7d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fa63498522e49ff87f7a51b40e73de6","placeholder":"​","style":"IPY_MODEL_10d65990246341a992070f351b636509","value":" 190/190 [00:00&lt;00:00, 14.9kB/s]"}},"9b9d39e6a7ab4df49949015bb1fe29e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2025da293b345f1b1f2d0edea9dc70a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8bd25946038461cbb779b09a8c030f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4a1567e9c414a91b2c55550effb4598":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"354b423edbc945639b5095fbb6caa506":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fa63498522e49ff87f7a51b40e73de6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10d65990246341a992070f351b636509":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acb48754bfe94ea2aaab5ce14551d6e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31f6d63611744d5d95b9cd673f1edf50","IPY_MODEL_6aff94b95a804c13bd031229a16488d6","IPY_MODEL_525f00f813bb4b769b27c3bd3ec28eb9"],"layout":"IPY_MODEL_a162c721b061403a87f1477124a7a0e5"}},"31f6d63611744d5d95b9cd673f1edf50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12b566b4a60e4061b0470b060d49404d","placeholder":"​","style":"IPY_MODEL_c89a3030e7924f4287f0c626c864a7fc","value":"tokenizer_config.json: 100%"}},"6aff94b95a804c13bd031229a16488d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb005d628435408c9e90cea9cf20c422","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc1d2bab1e2c46759ff4b6b6c8162ba2","value":48}},"525f00f813bb4b769b27c3bd3ec28eb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16a33ffb13a04fdd9a0258ac46b17f2d","placeholder":"​","style":"IPY_MODEL_98c947311bdc471e8c3c725a70e45bc6","value":" 48.0/48.0 [00:00&lt;00:00, 3.86kB/s]"}},"a162c721b061403a87f1477124a7a0e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12b566b4a60e4061b0470b060d49404d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89a3030e7924f4287f0c626c864a7fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb005d628435408c9e90cea9cf20c422":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc1d2bab1e2c46759ff4b6b6c8162ba2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16a33ffb13a04fdd9a0258ac46b17f2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c947311bdc471e8c3c725a70e45bc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d31602d6f594d7e9635c149bcefb0ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b032604078294c0b8a951aa3cb5fb511","IPY_MODEL_c453b991934e46728052fc331d3bf05e","IPY_MODEL_07e228916a754eae822c985e8916c0e1"],"layout":"IPY_MODEL_bb67d8d4e3b74b7ca3a9faeb97411398"}},"b032604078294c0b8a951aa3cb5fb511":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a827b8f7d8d74cb2868587f4e8424697","placeholder":"​","style":"IPY_MODEL_7f3fc2593eb246dfb403de19390a5dd5","value":"vocab.txt: 100%"}},"c453b991934e46728052fc331d3bf05e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1fe2bacc1dc44b38e049a73db21afb1","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_538d9d0f312f4e3795e1b0b27d89ede1","value":231508}},"07e228916a754eae822c985e8916c0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba924c386c454caa874456e81fa56b18","placeholder":"​","style":"IPY_MODEL_ae4e2654c9944c778995d73c629203c2","value":" 232k/232k [00:00&lt;00:00, 977kB/s]"}},"bb67d8d4e3b74b7ca3a9faeb97411398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a827b8f7d8d74cb2868587f4e8424697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f3fc2593eb246dfb403de19390a5dd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1fe2bacc1dc44b38e049a73db21afb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"538d9d0f312f4e3795e1b0b27d89ede1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba924c386c454caa874456e81fa56b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae4e2654c9944c778995d73c629203c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd9f7f55c3e3402ab474f08bae1effe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ffb72a960ea439a8795d20ae06794e0","IPY_MODEL_325d0ed8af474625a15506a99d024454","IPY_MODEL_7c71ca25bb1d465ebaee6b9e03e23dcf"],"layout":"IPY_MODEL_cf068308d1f14332a1cb766e351bb94f"}},"5ffb72a960ea439a8795d20ae06794e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c549dfacb9da46eeb1d3af3d8f563cbc","placeholder":"​","style":"IPY_MODEL_8ce0c6076bad4c7a924a6cfa8739e0df","value":"tokenizer.json: 100%"}},"325d0ed8af474625a15506a99d024454":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d88bddb4b69f4e7d88db6f097b4f716a","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_634b4f74841e4421b2abac7fb3da14d3","value":466062}},"7c71ca25bb1d465ebaee6b9e03e23dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3cb7aad4cc048df835d0bf98c92c2c2","placeholder":"​","style":"IPY_MODEL_4066c1837b0b4c9f8cfcb6cb09859407","value":" 466k/466k [00:00&lt;00:00, 1.02MB/s]"}},"cf068308d1f14332a1cb766e351bb94f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c549dfacb9da46eeb1d3af3d8f563cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ce0c6076bad4c7a924a6cfa8739e0df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d88bddb4b69f4e7d88db6f097b4f716a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634b4f74841e4421b2abac7fb3da14d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3cb7aad4cc048df835d0bf98c92c2c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4066c1837b0b4c9f8cfcb6cb09859407":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4aec0e35a5745ffba859b681f2c72c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_290fbd5cb787449b877df711d79d26de","IPY_MODEL_247ee70fea1b46ce9407dc905728f88e","IPY_MODEL_ccc982d382614579b5a54c1f3c120293"],"layout":"IPY_MODEL_ae559872afe04e88a78e96f4ab1c1e9c"}},"290fbd5cb787449b877df711d79d26de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad3c9661c85a4a1c8e6a9290dbd06eaa","placeholder":"​","style":"IPY_MODEL_66a8ed53f7f246df96e1ccf32a9086d0","value":"config.json: 100%"}},"247ee70fea1b46ce9407dc905728f88e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24eac9d9f35849d4a4061727d486d0e6","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f1e8db1e00a45d4b5315d75ab99e9be","value":483}},"ccc982d382614579b5a54c1f3c120293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb3b5ad7eb2408388267641b16168ef","placeholder":"​","style":"IPY_MODEL_37dc81cb367f49fea280b592ffe68131","value":" 483/483 [00:00&lt;00:00, 43.1kB/s]"}},"ae559872afe04e88a78e96f4ab1c1e9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3c9661c85a4a1c8e6a9290dbd06eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66a8ed53f7f246df96e1ccf32a9086d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24eac9d9f35849d4a4061727d486d0e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f1e8db1e00a45d4b5315d75ab99e9be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4cb3b5ad7eb2408388267641b16168ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37dc81cb367f49fea280b592ffe68131":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"029b11fd78d54e33843da3acf97d5f89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9275c325f58c408ab3df9efd7375a085","IPY_MODEL_3b6bca66ca14448d96e587d4b3c5c6d3","IPY_MODEL_713c57bf9c1e4c1ea83260ce112174d2"],"layout":"IPY_MODEL_61ed4bc7b480435dbef50ac9abac7270"}},"9275c325f58c408ab3df9efd7375a085":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_642ba189d983400998b5bbbe2f829e50","placeholder":"​","style":"IPY_MODEL_276f1687408f4338811772cb891dfebd","value":"model.safetensors: 100%"}},"3b6bca66ca14448d96e587d4b3c5c6d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e68e538fca64b4f9bc39eb8f5cefb57","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5044db1115d44cf3b84c6dc4d67378b6","value":267954768}},"713c57bf9c1e4c1ea83260ce112174d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4dc50e36b514a0da477ca818f282617","placeholder":"​","style":"IPY_MODEL_79ce1055ac7640fc8b6783cf512ea035","value":" 268M/268M [00:01&lt;00:00, 226MB/s]"}},"61ed4bc7b480435dbef50ac9abac7270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642ba189d983400998b5bbbe2f829e50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"276f1687408f4338811772cb891dfebd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e68e538fca64b4f9bc39eb8f5cefb57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5044db1115d44cf3b84c6dc4d67378b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4dc50e36b514a0da477ca818f282617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79ce1055ac7640fc8b6783cf512ea035":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e88a4cab91984054a3c2579460968cbb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe0e172f2b224564a3799b9f836e0498","IPY_MODEL_3a83e8fa5a3045969c583d6e43e4bfda","IPY_MODEL_5687a8ce93e54e029d9092aac62f38af"],"layout":"IPY_MODEL_2aefab9e4af64738981032fc9a19ba85"}},"fe0e172f2b224564a3799b9f836e0498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b39d8ab81864c51b8ad6a4591eb2866","placeholder":"​","style":"IPY_MODEL_ad040509b5fe43f5808580709a7c05d9","value":"Epoch 1/2 (Training): 100%"}},"3a83e8fa5a3045969c583d6e43e4bfda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a239185ff0c9450483baf19b85844283","max":1709,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3963aa23064e4530b247cc47e23acbbe","value":1709}},"5687a8ce93e54e029d9092aac62f38af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79366bea020a460190b60b43d27a3b82","placeholder":"​","style":"IPY_MODEL_914865dd3b494f05bf5b2460c753e5e5","value":" 1709/1709 [12:26&lt;00:00,  3.54it/s, Loss=0.6104, Accuracy=53.25%]"}},"2aefab9e4af64738981032fc9a19ba85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5b39d8ab81864c51b8ad6a4591eb2866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad040509b5fe43f5808580709a7c05d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a239185ff0c9450483baf19b85844283":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963aa23064e4530b247cc47e23acbbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79366bea020a460190b60b43d27a3b82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"914865dd3b494f05bf5b2460c753e5e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75087e4687764a808454c6a5557a4ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd1107616d21443e8f308c09c3db4152","IPY_MODEL_32e6631819314646824eded5ce16ddfe","IPY_MODEL_3c7f381552d4469c8e3dddc76803fd85"],"layout":"IPY_MODEL_877e7cb7cd804d8c9e73617b36e710b7"}},"bd1107616d21443e8f308c09c3db4152":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cece020b6a544c64aa436f19f5cc3e36","placeholder":"​","style":"IPY_MODEL_75c5756f8aee484881f565f33a09f38c","value":"Epoch 1/2 (Validation):   0%"}},"32e6631819314646824eded5ce16ddfe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f92db50834e3469bb54414142b2aab2a","max":428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c1ea37f5e0b45e7bceff0a50627cb3d","value":0}},"3c7f381552d4469c8e3dddc76803fd85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cf8ee68bdd54c6e96bbb75063613198","placeholder":"​","style":"IPY_MODEL_d5b7842737934112989806768283c7f3","value":" 0/428 [00:00&lt;?, ?it/s]"}},"877e7cb7cd804d8c9e73617b36e710b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cece020b6a544c64aa436f19f5cc3e36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75c5756f8aee484881f565f33a09f38c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f92db50834e3469bb54414142b2aab2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c1ea37f5e0b45e7bceff0a50627cb3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cf8ee68bdd54c6e96bbb75063613198":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5b7842737934112989806768283c7f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjXSi6z_jqpX","executionInfo":{"status":"ok","timestamp":1734007695703,"user_tz":-60,"elapsed":51894,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"65bd8b79-465c-452a-f635-8a91ac1f80cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["\n","import numpy as np\n","import pandas as pd\n","\n","df_n = np.load('drive/MyDrive/cleaned_train_dataset_megafinal_processed.npy', allow_pickle=True)\n","df = pd.DataFrame(df_n)"],"metadata":{"id":"OhSe9hYzE56A","executionInfo":{"status":"ok","timestamp":1734007698550,"user_tz":-60,"elapsed":2849,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df[2].apply(len).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNrK0W-IFkPG","executionInfo":{"status":"ok","timestamp":1734007698550,"user_tz":-60,"elapsed":6,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"bd4ae924-677b-40dc-ffaa-f45d6a2b3165"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45782.897051941974"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df_eval_n = np.load('drive/MyDrive/cleaned_eval_dataset_training.npy', allow_pickle=True)\n","df_eval = pd.DataFrame(df_eval_n)"],"metadata":{"id":"nR0sM8KXFon2","executionInfo":{"status":"ok","timestamp":1734007699997,"user_tz":-60,"elapsed":1451,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["df_eval[1].apply(len).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vZfGHT3F-25","executionInfo":{"status":"ok","timestamp":1734007699998,"user_tz":-60,"elapsed":6,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"ea5b87fc-d2da-46da-c52a-33f05c205fad"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["44984.56007751938"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import transformers\n","from sentence_transformers import SentenceTransformer\n","from transformers import DistilBertForSequenceClassification, DistilBertConfig\n","\n","from sentence_transformers import SentenceTransformer\n","import torch\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n","\n","# Load pre-trained Sentence-BERT model (outputs 768 embeddings)\n","# sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n","sentence_model = SentenceTransformer('all-mpnet-base-v2')\n","\n","# Load the DistilBert tokenizer (to ensure correct tokenization)\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Example long text\n","long_text = \"This is a very long sentence. \" * 50  # Make the text longer than 512 tokens for the example\n","\n","\n","# Define custom embedding layer for the DistilBert model\n","class CustomSentenceEmbeddingModel(torch.nn.Module):\n","    def __init__(self, sentence_model):\n","        super(CustomSentenceEmbeddingModel, self).__init__()\n","        self.sentence_model = sentence_model\n","\n","    def forward(self, text):\n","\n","        sentences = self.divide_tweet(text)\n","        return self.sentence_model.encode(sentences, convert_to_tensor=True)\n","\n","\n","    def divide_tweet(self, tweet, num_parts=512):\n","        \"\"\"\n","        Divide a tweet into 512 equal-length parts\n","\n","        Args:\n","            tweet (str): Input tweet text\n","            num_parts (int): Number of parts to divide the tweet into\n","\n","        Returns:\n","            list: List of tweet parts\n","        \"\"\"\n","        # Calculate the length of each part\n","        tweet_length = len(tweet)\n","        part_length = max(1, tweet_length // num_parts)\n","\n","        # Divide the tweet into parts\n","        parts = [\n","            tweet[i:i+part_length]\n","            for i in range(0, tweet_length, part_length)\n","        ]\n","\n","        # Ensure we have exactly 512 parts, padding or truncating if necessary\n","        if len(parts) > num_parts:\n","            parts = parts[:num_parts]\n","        elif len(parts) < num_parts:\n","            # Pad with empty strings\n","            parts.extend([''] * (num_parts - len(parts)))\n","\n","        return parts\n","\n","# Load the DistilBert model for sequence classification\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","print(model)\n","\n","# Replace the embeddings layer with the custom Sentence-BERT model\n","model.distilbert.embeddings = CustomSentenceEmbeddingModel(sentence_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["382ad59b03994c70a67a2ae2f287ab6a","2fbad95b314a4688ae760ead39b8e6bb","4b089aab0f664430a905ebc84564eabf","7b5ac9e0ef0c494fbc8d157740ea666f","92503f02313c4a9a96681352ed111087","743e561bef204a11ad4b8e68cffc6a49","1ce0b25e26fe4969b3a58f8c77b4f49e","84b68c8930ee4e4bae708e732320600d","582e99fc47a8426892cd22d2eb0710dd","1e74222553024b369d825dc0bd521f89","fc81161314d8404b8a24100065f08303","f3b15795e34747ab95e35d511db927b6","19b721a83a7b48fba2237e50c5fb1515","ee1240838b774875a794970b5d47513b","b8bdf377a77d41938b7de3cbce3bf97c","b17ad4f673c04380bf2b8b8a0abbc8e7","ea966cf1608f4a1faee0063488898819","6716188e90104a2b8369d0589f09207b","ab87af2601544f9a9482906654ff8059","578704c4e95e4f6c913f03b869f3b347","f580e9fcb671494fa20ca3962ec2ebe9","112cad1abfc347648c55edf4dc4f13d3","a0fa18ee4151496789660176a71daabb","671157d41bb449b68661b8cfaf0aff76","cce5ef96020a4ae6b04cc7b9bcff633d","ff85becf6fdc40f1bd9cf5cc03593056","a1ec9513622a4863a221e6dee10afaf8","80326ab256274b8ab7e6e26b3785d97e","656d169134a34419a64b871dd257316b","d7408c18d0d5413d986529f271abf48d","a46b5ac422f3476db8cacbace4d6e123","106db31c469b40189c97e9052f618ba2","35c12ae63dff4f1c83765aebcb4c31b1","0e6edc6c21764a73acaa4508acfeb492","91e48ac0fb5b45bb91af06f0beea6023","7a4b4358260b4c37882772e389f5ca57","f23a1d5f6c2c4cb78ea003a45d952a9e","37340df537c7473fb98e3afad2e86cc5","4957cc48b7e743cab3fe8faac0529c60","c83a219426764bc4913e016a333ba2be","dfcc1b43be5944068825e46533369b88","9d8fbe4a03404727a1960c673af2fcdc","e8bd0398573d4905be7c90a77c36071a","2a8951ab586d4fd2b64a80257231cbe4","0234946a8ec945ac83a4993ef369ef1d","33c3aa67fcf84fc497b88204375f65fb","f052f450d5a7498ca2988e075ea77607","0815906440cc4b44a29bb1da4400e46c","7ba4c15b23cb4bfca7ac4a7a4c4e8720","3143dafe734e4f6387eccb34729058f4","698fee7f2fa945e195d73b3fb26e2c50","6fa2d99070ce431f927dd089697218f0","8c397a43dac1492fb37d2859040cb962","11e1dd68d7424660962b5418561a182c","447e2b52e7294966b2338f50184ffe1d","1b75c8e9c7354228aed1cc2e493a35cd","5ed83ce3d6dc4547af5094edd81018eb","919ebc9172584453b41badc9246466f3","dca4dd78c43d41e79d2dda5d0028f48c","b96f10256846400296feeef200a1f7e0","b934a5cbd02843b0b933889e86080c61","bfcd15baa28347d4ac5526a0fd46b7e4","2dd3773e61594ab5a53b6d34e9d43535","4d639e8a1e0845ca97a4d3811251b93f","fe5b6852419040c38743e9b4b388bf6f","ab551537a0134954952321cdd581d927","8473d3a033744ca981c2d9ea6eaa95ce","decbc39910af4b9487c506388269837d","3fe4baa29f28417a9e119cc2f1f96495","25437d0d3f0047468f70f0279d7acc1c","94814c38c1ec49c48c393d33d7f3aa91","e6650384426b41e9909d5758721ac88a","8b9a93607e594f329b9ffac086371085","b4079dc205314fbd930a9cdba8e620f3","2401e19646df4a8988f23e9ef126cebe","8e83f6a0098444b5b94850edab5cc290","04463fadfe1f4298b198634caf5b24b3","36eec1703124414fbd179b29f67c61b3","b6917fb66d9c4b0c907d08071c0f698c","692a6aafb51d4bb491ea8c1f2154640c","f6b96c6ca56446aaaf2d6c9d20870021","e6b913eb42fb47a4a2b475a6e978e9bc","6777c6da2d294de7814c2e57f13e87f2","508b6398949946a5a8ae9d68b2d6f3b9","3999e126b65748b1a601410ff942adb0","5876d496013f4ebfb9ff98b1a3bc16b2","d81281b9bc434d3bb0d187fd8b658da8","dec0c24b5d4b4e3f859ae2c184ebcb37","d211dad6624d410c9159419f71246de9","7ac34ef3f13648ba9235aa2f364ecd7e","cb5a4322fadc45e29386dfa76f773073","4a607deeed254fd1ae925479023ff243","efcd7134bac741889f2319053d485dff","5a6d04239c1b4a818777a19089ecf4ce","80e89d31ea2648ac952dfd38ee9c3219","31ac8d2876b34a668ed74642e692e212","e147f2aabba84516ad21449ba492f8ca","1dde854b439d4e15ac6b799ad809b868","23b603ba04694577a96f150820e620f0","b42db315abc34d22a5f6b5719330093d","722c03dfca0f4616a81166a74a244912","92605a1f24484f2b80664cb273779e14","c231eecf50ab4740ba553f357a719a95","b60fe42230294f179e429696ba370615","91e3887e185f458da4cc87dd67c41fe6","814e89ec5623441fa2f99be78e2d14cf","f227d6f720c34b09a403627b89b56514","2b7df8c185af41f29890f98acd81ed91","4ffee34a0e2a4be4aca3d08a22132a66","d686612782e0494cb71ec58015f4cc61","dd06b5befe07446aa981c750976b6310","04e3e1eadbcb4788b35677245c9d2901","2222386db32e421e9ec481d09e3d9bf0","b2a195c0ff64448db15d8df7d1f1a7d6","9b9d39e6a7ab4df49949015bb1fe29e9","b2025da293b345f1b1f2d0edea9dc70a","b8bd25946038461cbb779b09a8c030f4","b4a1567e9c414a91b2c55550effb4598","354b423edbc945639b5095fbb6caa506","7fa63498522e49ff87f7a51b40e73de6","10d65990246341a992070f351b636509","acb48754bfe94ea2aaab5ce14551d6e1","31f6d63611744d5d95b9cd673f1edf50","6aff94b95a804c13bd031229a16488d6","525f00f813bb4b769b27c3bd3ec28eb9","a162c721b061403a87f1477124a7a0e5","12b566b4a60e4061b0470b060d49404d","c89a3030e7924f4287f0c626c864a7fc","eb005d628435408c9e90cea9cf20c422","fc1d2bab1e2c46759ff4b6b6c8162ba2","16a33ffb13a04fdd9a0258ac46b17f2d","98c947311bdc471e8c3c725a70e45bc6","4d31602d6f594d7e9635c149bcefb0ef","b032604078294c0b8a951aa3cb5fb511","c453b991934e46728052fc331d3bf05e","07e228916a754eae822c985e8916c0e1","bb67d8d4e3b74b7ca3a9faeb97411398","a827b8f7d8d74cb2868587f4e8424697","7f3fc2593eb246dfb403de19390a5dd5","f1fe2bacc1dc44b38e049a73db21afb1","538d9d0f312f4e3795e1b0b27d89ede1","ba924c386c454caa874456e81fa56b18","ae4e2654c9944c778995d73c629203c2","cd9f7f55c3e3402ab474f08bae1effe2","5ffb72a960ea439a8795d20ae06794e0","325d0ed8af474625a15506a99d024454","7c71ca25bb1d465ebaee6b9e03e23dcf","cf068308d1f14332a1cb766e351bb94f","c549dfacb9da46eeb1d3af3d8f563cbc","8ce0c6076bad4c7a924a6cfa8739e0df","d88bddb4b69f4e7d88db6f097b4f716a","634b4f74841e4421b2abac7fb3da14d3","f3cb7aad4cc048df835d0bf98c92c2c2","4066c1837b0b4c9f8cfcb6cb09859407","e4aec0e35a5745ffba859b681f2c72c1","290fbd5cb787449b877df711d79d26de","247ee70fea1b46ce9407dc905728f88e","ccc982d382614579b5a54c1f3c120293","ae559872afe04e88a78e96f4ab1c1e9c","ad3c9661c85a4a1c8e6a9290dbd06eaa","66a8ed53f7f246df96e1ccf32a9086d0","24eac9d9f35849d4a4061727d486d0e6","3f1e8db1e00a45d4b5315d75ab99e9be","4cb3b5ad7eb2408388267641b16168ef","37dc81cb367f49fea280b592ffe68131","029b11fd78d54e33843da3acf97d5f89","9275c325f58c408ab3df9efd7375a085","3b6bca66ca14448d96e587d4b3c5c6d3","713c57bf9c1e4c1ea83260ce112174d2","61ed4bc7b480435dbef50ac9abac7270","642ba189d983400998b5bbbe2f829e50","276f1687408f4338811772cb891dfebd","0e68e538fca64b4f9bc39eb8f5cefb57","5044db1115d44cf3b84c6dc4d67378b6","b4dc50e36b514a0da477ca818f282617","79ce1055ac7640fc8b6783cf512ea035"]},"id":"tKRXkLyLni3w","executionInfo":{"status":"ok","timestamp":1734007733327,"user_tz":-60,"elapsed":33332,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"2ccb9f82-5cf9-42b3-ffa8-93317dcc75d3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"382ad59b03994c70a67a2ae2f287ab6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b15795e34747ab95e35d511db927b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0fa18ee4151496789660176a71daabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6edc6c21764a73acaa4508acfeb492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0234946a8ec945ac83a4993ef369ef1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b75c8e9c7354228aed1cc2e493a35cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8473d3a033744ca981c2d9ea6eaa95ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36eec1703124414fbd179b29f67c61b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d211dad6624d410c9159419f71246de9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42db315abc34d22a5f6b5719330093d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd06b5befe07446aa981c750976b6310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb48754bfe94ea2aaab5ce14551d6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d31602d6f594d7e9635c149bcefb0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9f7f55c3e3402ab474f08bae1effe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4aec0e35a5745ffba859b681f2c72c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029b11fd78d54e33843da3acf97d5f89"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): DistilBertSdpaAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from sentence_transformers import SentenceTransformer\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n","\n","# Load pre-trained Sentence-BERT model (outputs 768 embeddings)\n","sentence_model = SentenceTransformer('all-mpnet-base-v2')\n","\n","# Load the DistilBert tokenizer (to ensure correct tokenization)\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Example long text\n","long_text = \"This is a very long sentence. \" * 50  # Make the text longer than 512 tokens for the example\n","\n","# Define custom embedding layer for the DistilBert model\n","class CustomSentenceEmbeddingModel(torch.nn.Module):\n","    def __init__(self, sentence_model):\n","        super(CustomSentenceEmbeddingModel, self).__init__()\n","        self.sentence_model = sentence_model\n","\n","    def forward(self, text):\n","        sentences = self.divide_tweet(text)\n","        return self.sentence_model.encode(sentences, convert_to_tensor=True)\n","\n","    def divide_tweet(self, tweet, num_parts=512):\n","        \"\"\"\n","        Divide a tweet into 512 equal-length parts\n","\n","        Args:\n","            tweet (str): Input tweet text\n","            num_parts (int): Number of parts to divide the tweet into\n","\n","        Returns:\n","            list: List of tweet parts\n","        \"\"\"\n","        tweet_length = len(tweet)\n","        part_length = max(1, tweet_length // num_parts)\n","\n","        # Divide the tweet into parts\n","        parts = [\n","            tweet[i:i+part_length]\n","            for i in range(0, tweet_length, part_length)\n","        ]\n","\n","        # Ensure we have exactly 512 parts, padding or truncating if necessary\n","        if len(parts) > num_parts:\n","            parts = parts[:num_parts]\n","        elif len(parts) < num_parts:\n","            # Pad with empty strings\n","            parts.extend([''] * (num_parts - len(parts)))\n","\n","        return parts\n","\n","# Load the DistilBert model for sequence classification\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","# Print model architecture (for verification)\n","print(\"DistilBert model loaded:\", model)\n","\n","# Replace the embeddings layer with the custom Sentence-BERT model\n","model.distilbert.embeddings = CustomSentenceEmbeddingModel(sentence_model)\n","\n","\n","def hook_fn(module, input, output):\n","    print(f\"Module name: {module.__class__.__name__}, Output shape: {output.shape}\")\n","\n","# Assuming 'model' is your PyTorch model\n","for name, module in model.named_modules():\n","    module.register_forward_hook(hook_fn)\n","\n","\n","# Test the chunking function\n","def test_chunking():\n","    sentences = model.distilbert.embeddings.divide_tweet(long_text)\n","    assert len(sentences) == 512, f\"Expected 512 parts, got {len(sentences)}\"\n","    print(\"Chunking test passed. Number of chunks:\", len(sentences))\n","\n","# Test Sentence-BERT embedding output\n","def test_embedding_output():\n","    sentences = model.distilbert.embeddings.divide_tweet(long_text)\n","    embeddings = model.distilbert.embeddings.sentence_model.encode(sentences, convert_to_tensor=True)\n","\n","    # Check if embeddings are generated and have the correct shape\n","    assert embeddings.shape == (512, 768), f\"Expected embedding shape (512, 768), but got {embeddings.shape}\"\n","    print(\"Embedding output test passed. Shape:\", embeddings.shape)\n","\n","# Test model forward pass\n","def test_model_forward():\n","\n","    sentences = model.distilbert.embeddings.divide_tweet(long_text)\n","    embeddings = model.distilbert.embeddings.sentence_model.encode(sentences, convert_to_tensor=True)\n","\n","    # Make a dummy batch of embeddings (add batch dimension)\n","    batch_embeddings = embeddings.unsqueeze(0)  # Shape: (1, 512, 768)\n","\n","    # Forward pass through the classification model\n","\n","\n","\n","    outputs = model.pre_classifier(batch_embeddings)\n","    print(outputs.shape)\n","    outputs = model.classifier(outputs)  # Final logits\n","    assert outputs.shape == (1, 2), f\"Expected output shape (1, 2), got {outputs.shape}\"\n","    print(\"Model forward pass test passed. Logits shape:\", outputs.shape)\n","\n","# Run the tests\n","test_chunking()\n","test_embedding_output()\n","test_model_forward()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-sV_vUdpzviA","executionInfo":{"status":"error","timestamp":1734007737558,"user_tz":-60,"elapsed":4251,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"46c03309-a268-46e5-ba83-249f0aab9f67"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["DistilBert model loaded: DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): DistilBertSdpaAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n","Chunking test passed. Number of chunks: 512\n","Module name: Embedding, Output shape: torch.Size([32, 3, 768])\n","Module name: Embedding, Output shape: torch.Size([32, 3, 768])\n","Module name: LayerNorm, Output shape: torch.Size([32, 3, 768])\n","Module name: Dropout, Output shape: torch.Size([32, 3, 768])\n","Module name: MPNetEmbeddings, Output shape: torch.Size([32, 3, 768])\n","Module name: Embedding, Output shape: torch.Size([3, 3, 12])\n","Module name: Linear, Output shape: torch.Size([32, 3, 768])\n","Module name: Linear, Output shape: torch.Size([32, 3, 768])\n","Module name: Linear, Output shape: torch.Size([32, 3, 768])\n","Module name: Dropout, Output shape: torch.Size([32, 12, 3, 3])\n","Module name: Linear, Output shape: torch.Size([32, 3, 768])\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'tuple' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-6889713a53d1>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Run the tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mtest_chunking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mtest_embedding_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mtest_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-6889713a53d1>\u001b[0m in \u001b[0;36mtest_embedding_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_embedding_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Check if embeddings are generated and have the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mmodule_kwarg_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mmodule_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_kwarg_keys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mpnet/modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mpnet/modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mpnet/modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     ):\n\u001b[0;32m--> 293\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mpnet/modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         self_outputs = self.attn(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-6889713a53d1>\u001b[0m in \u001b[0;36mhook_fn\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module name: {module.__class__.__name__}, Output shape: {output.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Assuming 'model' is your PyTorch model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from sentence_transformers import SentenceTransformer\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n","\n","# Load pre-trained Sentence-BERT model (outputs 768 embeddings)\n","sentence_model = SentenceTransformer('all-mpnet-base-v2')\n","\n","# Load the DistilBert tokenizer (to ensure correct tokenization)\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Define custom embedding layer for the DistilBert model\n","import torch\n","import torch.nn as nn\n","from transformers import DistilBertForSequenceClassification\n","\n","# Custom Sentence Embedding Model\n","class CustomSentenceEmbeddingModel(torch.nn.Module):\n","    def __init__(self, sentence_model):\n","        super(CustomSentenceEmbeddingModel, self).__init__()\n","        self.sentence_model = sentence_model\n","        self.cls_embedding = nn.Parameter(torch.randn(1, 768))  # learnable CLS embedding\n","\n","    def forward(self, text):\n","        sentences = self.divide_tweet(text)\n","        sentence_embeddings = self.sentence_model.encode(sentences, convert_to_tensor=True).unsqueeze(0)\n","        # print(sentence_embeddings.shape)\n","        # Concatenate CLS embedding with sentence embeddings\n","        # Here, we assume the `sentence_embeddings` is a tensor, and we want to add the CLS token to it\n","        cls_embeddings = self.cls_embedding.expand(sentence_embeddings.size(0), -1).unsqueeze(0)  # Match the batch size\n","        # print(cls_embeddings.shape)\n","        sentence_embeddings_with_cls = torch.cat([cls_embeddings, sentence_embeddings], dim=1)  # Concatenate along the feature axis\n","        # print(sentence_embeddings_with_cls.shape)\n","        return sentence_embeddings_with_cls # Freeze to avoid updatessentence_embeddings_with_cls\n","\n","    def divide_tweet(self, tweet, num_parts=511):\n","        \"\"\"\n","        Divide a tweet into 512 equal-length parts\n","\n","        Args:\n","            tweet (str): Input tweet text\n","            num_parts (int): Number of parts to divide the tweet into\n","\n","        Returns:\n","            list: List of tweet parts\n","        \"\"\"\n","        tweet_length = len(tweet)\n","        part_length = max(1, tweet_length // num_parts)\n","\n","        parts = [\n","            tweet[i:i + part_length]\n","            for i in range(0, tweet_length, part_length)\n","        ]\n","\n","        if len(parts) > num_parts:\n","            parts = parts[:num_parts]\n","        elif len(parts) < num_parts:\n","            parts.extend([''] * (num_parts - len(parts)))\n","\n","        return parts\n","\n","class CustomDistilBERTModel(nn.Module):\n","    def __init__(self, sentence_model, num_labels=2):\n","        super(CustomDistilBERTModel, self).__init__()\n","\n","        # Load DistilBERT pre-trained model for sequence classification\n","        self.distilbert = DistilBertForSequenceClassification.from_pretrained(\n","            \"distilbert-base-uncased\", num_labels=num_labels\n","        )\n","\n","        # Replace the embeddings layer with nn.Identity\n","        self.distilbert.distilbert.embeddings = nn.Identity()\n","\n","        # Replace the embeddings layer with the custom Sentence-BERT model\n","        self.new_embeds = CustomSentenceEmbeddingModel(sentence_model)\n","\n","        # Override the classifier\n","        self.classifier = nn.Linear(self.distilbert.config.hidden_size, num_labels)  # Adjusted for no extra CLS embedding\n","\n","    def forward(self, text, labels):\n","        # Get the custom embeddings from the Sentence-BERT model\n","        embeddings = self.new_embeds(text).long()\n","        # print(\"Embeddings shape:\", embeddings.shape)\n","\n","        # Pass embeddings through the transformer layers of DistilBERT\n","        outputs = self.distilbert.distilbert(embeddings)  # Get output from DistilBERT transformer layers\n","        embeddings = outputs.last_hidden_state  # Extract hidden states from the transformer\n","\n","        # print(\"DistilBERT transformer output shape:\", embeddings.shape)\n","\n","        # Use the CLS token representation directly from the transformer output\n","        cls_representation = embeddings[:, 0, :]  # First token (CLS token) from transformer output\n","\n","        # Pass the CLS representation through the classifier\n","        logits = self.classifier(cls_representation)\n","\n","        return logits\n","\n","\n","class CustomDistilBERTModelSimple(nn.Module):\n","    def __init__(self, sentence_model, num_labels=2):\n","        super(CustomDistilBERTModelSimple, self).__init__()\n","\n","        # Load DistilBERT pre-trained model for sequence classification\n","        self.distilbert = DistilBertForSequenceClassification.from_pretrained(\n","            \"distilbert-base-uncased\", num_labels=num_labels\n","        )\n","\n","        # Replace the embeddings layer with nn.Identity\n","        # self.distilbert.distilbert.embeddings = nn.Identity()\n","\n","        # Replace the embeddings layer with the custom Sentence-BERT model\n","        self.new_embeds = CustomSentenceEmbeddingModel(sentence_model)\n","\n","        # Override the classifier\n","        self.classifier = nn.Linear(self.distilbert.config.hidden_size, num_labels)  # Adjusted for no extra CLS embedding\n","\n","    def forward(self, text, labels=None):\n","        # Get the custom embeddings from the Sentence-BERT model\n","        embeddings = self.new_embeds(text).long()\n","        # print(\"Embeddings shape:\", embeddings.shape)\n","\n","        B, L, T = embeddings.shape\n","\n","        # Pass embeddings through the transformer layers of DistilBERT\n","        input_ids = torch.arange(L).unsqueeze(0).repeat(B, 1)\n","        outputs = self.distilbert(inputs_embeds = embeddings, labels=labels)  # Get output from DistilBERT transformer layers\n","        # embeddings = outputs.last_hidden_state  # Extract hidden states from the transformer\n","        # embeddings = outputs\n","\n","        # Use the CLS token representation directly from the transformer output\n","        # cls_representation = embeddings[:, 0, :]  # First token (CLS token) from transformer output\n","\n","        # Pass the CLS representation through the classifier\n","        # logits = self.classifier(cls_representation)\n","        # print(outputs)\n","        logits = outputs.logits\n","        return outputs\n","\n","\n","sentence_model = SentenceTransformer('all-mpnet-base-v2')\n","# Instantiate the custom model\n","model = CustomDistilBERTModelSimple(sentence_model)\n","\n","# Example input\n","long_text = \"This is a very long sentence. \" * 50 # Make the text longer than 512 tokens for the example\n","# text = [\"This is a ve\", \"awfdawfawf\"]\n","# inputs = tokenizer(long_text, return_tensors=\"pt\", truncation=True, padding=True)\n","\n","# # Forward pass through the custom model\n","# output = model(long_text, torch.tensor([1]))\n","# print(output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOnJ-ssx-TxY","executionInfo":{"status":"ok","timestamp":1734007754817,"user_tz":-60,"elapsed":6384,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"ceb1e038-f307-4665-bf8a-5a6179825a42"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels, max_length=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        # Tokenize the text and pad/truncate it to the max length\n","        # We need to squeeze out the extra dimension that `return_tensors=\"pt\"` adds\n","        # input_ids = encoding[\"input_ids\"].squeeze(0)  # shape (seq_length,)\n","        # attention_mask = encoding[\"attention_mask\"].squeeze(0)  # shape (seq_length,)\n","\n","        return text, label\n","\n","# Step 3: Create the Dataset\n","dataset = TextDataset(df[2], df[1])"],"metadata":{"id":"PtAY3ygErbVY","executionInfo":{"status":"ok","timestamp":1734007766797,"user_tz":-60,"elapsed":895,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback\n",")\n","from tqdm.auto import tqdm  # Added tqdm import\n","\n","import torch.optim as optim\n","import torch.nn as nn\n","import random\n","\n","class TweetSentenceClassifier:\n","    def __init__(self, model_name='distilbert-base-uncased', sentence_model=SentenceTransformer('all-mpnet-base-v2'), num_labels=2):\n","        \"\"\"\n","        Initialize the Tweet Sentence Classifier\n","\n","        :param model_name: Pretrained model to use as base\n","        :param num_labels: Number of classification categories\n","        \"\"\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        # If num_labels is not specified, it will be inferred from the dataset\n","        self.num_labels = num_labels\n","        self.model = CustomDistilBERTModelSimple(sentence_model).to(self.device)\n","\n","    def prepare_dataset(self, df, text_column, label_column, test_size=0.2, random_state=42):\n","        \"\"\"\n","        Prepare dataset for training\n","\n","        :param df: Pandas DataFrame containing tweets\n","        :param text_column: Name of column with tweet text\n","        :param label_column: Name of column with labels\n","        :param test_size: Proportion of dataset to use for testing\n","        :param random_state: Random seed for reproducibility\n","        :return: Tuple of train and test datasets\n","        \"\"\"\n","        # Infer number of labels if not specified\n","        if self.num_labels is None:\n","            self.num_labels = len(df[label_column].unique())\n","\n","        # Initialize model with correct number of labels\n","\n","\n","        # Tokenization function\n","        def tokenize_function(text):\n","            return self.tokenizer(\n","                text,\n","                padding='max_length',\n","                truncation=True,\n","                max_length=512\n","            )\n","\n","        # Split the data\n","        train_df, test_df = train_test_split(\n","            df,\n","            test_size=test_size,\n","            random_state=random_state,\n","        )\n","\n","        train_df.reset_index(drop=True, inplace=True)\n","        test_df.reset_index(drop=True, inplace=True)\n","\n","        # Tokenize datasets\n","        # train_df['tokenized'] = train_df[text_column].apply(tokenize_function)\n","        # test_df['tokenized'] = test_df[text_column].apply(tokenize_function)\n","        self.test_data = TextDataset(test_df[text_column], train_df[label_column])\n","        self.train_data = TextDataset(train_df[text_column], train_df[label_column])\n","        # return train_df, test_df\n","\n","    def prepare_general_dataset(self, df, text_column, label_column, test_size=0.2, random_state=42):\n","        \"\"\"\n","        Prepare general dataset for training\n","\n","        :param df: Pandas DataFrame containing tweets\n","        :param text_column: Name of column with tweet text\n","        :param label_column: Name of column with labels\n","        :param test_size: Proportion of dataset to use for testing\n","        :param random_state: Random seed for reproducibility\n","        :return: Tuple of train and test datasets\n","        \"\"\"\n","        # Infer number of labels if not specified\n","        if self.num_labels is None:\n","            self.num_labels = len(df[label_column].unique())\n","\n","\n","        # Tokenization function\n","        # def tokenize_function(text):\n","        #     return self.tokenizer(\n","        #         text,\n","        #         truncation=True,\n","        #         max_length=len(text)*10\n","        #     )\n","\n","        # Split the data\n","        train_df, test_df = train_test_split(\n","            df,\n","            test_size=test_size,\n","            random_state=random_state,\n","        )\n","\n","        # Tokenize datasets\n","        # train_df['tokenized'] = train_df[text_column].apply(tokenize_function)\n","        # test_df['tokenized'] = test_df[text_column].apply(tokenize_function)\n","\n","        return train_df, test_df\n","\n","\n","    def prepare_loaders_from_generals(self, train_df, test_df, label_column, max_length=512):\n","        def process_tokens(tokenized_items):\n","            processed_input_ids = []\n","            processed_attention_masks = []\n","\n","            for item in tokenized_items:\n","                input_ids = item['input_ids']\n","                attention_mask = item['attention_mask']\n","\n","                # If sequence is shorter than 512, pad\n","                if len(input_ids) <= max_length:\n","                    pad_length = max_length - len(input_ids)\n","                    padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_length\n","                    padded_attention_mask = attention_mask + [0] * pad_length\n","                    processed_input_ids.append(padded_input_ids)\n","                    processed_attention_masks.append(padded_attention_mask)\n","\n","                # If sequence is longer than 512, choose random start\n","                else:\n","                    # Calculate the maximum possible start index\n","                    max_start_index = len(input_ids) - max_length\n","\n","                    # Choose a random start index\n","                    start_index = random.randint(0, max_start_index)\n","\n","                    # Extract 512 consecutive tokens\n","                    windowed_input_ids = input_ids[start_index:start_index + max_length]\n","                    windowed_attention_mask = attention_mask[start_index:start_index + max_length]\n","\n","                    processed_input_ids.append(windowed_input_ids)\n","                    processed_attention_masks.append(windowed_attention_mask)\n","\n","            return processed_input_ids, processed_attention_masks\n","\n","        # Ensure random seed for reproducibility\n","        # random.seed(42)\n","\n","        # Process train and test data\n","        train_input_ids, train_attention_mask = process_tokens(train_df['tokenized'])\n","        test_input_ids, test_attention_mask = process_tokens(test_df['tokenized'])\n","\n","        # Convert to tensors\n","        train_input_ids = torch.tensor(train_input_ids)\n","        train_attention_mask = torch.tensor(train_attention_mask)\n","        # train_labels = torch.tensor(train_df[label_column].values)\n","\n","        test_input_ids = torch.tensor(test_input_ids)\n","        test_attention_mask = torch.tensor(test_attention_mask)\n","        # test_labels = torch.tensor(test_df[label_column].values)\n","\n","        train_labels = torch.tensor(np.array(train_df[label_column].to_list(), dtype=np.int64), dtype=torch.long)\n","        test_labels = torch.tensor(np.array(test_df[label_column].to_list(), dtype=np.int64), dtype=torch.long)\n","\n","        # Create data loaders\n","        train_dataset = torch.utils.data.TensorDataset(\n","            train_input_ids,\n","            train_attention_mask,\n","            train_labels\n","        )\n","        test_dataset = torch.utils.data.TensorDataset(\n","            test_input_ids,\n","            test_attention_mask,\n","            test_labels\n","        )\n","\n","        train_loader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=32,\n","            shuffle=False\n","        )\n","        test_loader = torch.utils.data.DataLoader(\n","            test_dataset,\n","            batch_size=32,\n","            shuffle=False\n","        )\n","\n","        return train_loader, test_loader\n","\n","\n","    def compute_metrics(self, pred):\n","        \"\"\"\n","        Compute evaluation metrics\n","\n","        :param pred: Predictions from the model\n","        :return: Dictionary of metrics\n","        \"\"\"\n","        labels = pred.label_ids\n","        preds = pred.predictions.argmax(-1)\n","\n","        # Compute classification report\n","        report = classification_report(labels, preds, output_dict=True)\n","\n","        return {\n","            'accuracy': report['accuracy'],\n","            'macro_f1': report['macro avg']['f1-score'],\n","            'weighted_f1': report['weighted avg']['f1-score']\n","        }\n","\n","    def custom_loader_train(self, train_df, test_df, label_column, epochs=1, learning_rate=2e-5):\n","        \"\"\"\n","        Custom training method with tqdm progress bars\n","\n","        :param train_df: Training dataframe with tokenized column\n","        :param test_df: Test dataframe with tokenized column\n","        :param label_column: Name of the label column\n","        :param epochs: Number of training epochs\n","        :param learning_rate: Learning rate for optimizer\n","        \"\"\"\n","\n","        # test_input_ids = torch.tensor([x['input_ids'] for x in test_df['tokenized']])\n","        # test_attention_mask = torch.tensor([x['attention_mask'] for x in test_df['tokenized']])\n","        # test_labels = torch.tensor(test_df[label_column].values)\n","\n","        # # Create data loaders\n","\n","        # test_dataset = torch.utils.data.TensorDataset(\n","        #     test_input_ids,\n","        #     test_attention_mask,\n","        #     test_labels\n","        # )\n","\n","        # test_loader = torch.utils.data.DataLoader(\n","        #     test_dataset,\n","        #     batch_size=32,\n","        #     shuffle=False\n","        # )\n","\n","\n","\n","        # Prepare optimizer and loss\n","        optimizer = optim.AdamW(\n","            self.model.parameters(),\n","            lr=learning_rate\n","        )\n","        criterion = nn.CrossEntropyLoss()\n","\n","        # Training loop with tqdm progress bars\n","        for epoch in range(epochs):\n","            # Training phase\n","            self.model.train()\n","            train_loss = 0\n","            train_correct = 0\n","            train_total = 0\n","\n","            train_loader, test_loader = self.prepare_loaders_from_generals(train_df, test_df, label_column)\n","\n","\n","            # Wrap train_loader with tqdm for progress bar\n","            train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Training)\", leave=False)\n","            for batch in train_progress_bar:\n","                # Unpack batch\n","                input_ids, attention_mask, labels = [b.to(self.device) for b in batch]\n","\n","                # Zero gradients\n","                optimizer.zero_grad()\n","\n","                # Forward pass\n","                outputs = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    labels=labels\n","                )\n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","                # Backward pass\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Compute accuracy\n","                train_loss += loss.item()\n","                _, predicted = torch.max(logits, 1)\n","                train_total += labels.size(0)\n","                train_correct += (predicted == labels).sum().item()\n","\n","                # Update progress bar\n","                train_progress_bar.set_postfix({\n","                    'Loss': f'{loss.item():.4f}',\n","                    'Accuracy': f'{100 * train_correct / train_total:.2f}%'\n","                })\n","\n","            # Validation phase\n","            self.model.eval()\n","            val_loss = 0\n","            val_correct = 0\n","            val_total = 0\n","            all_preds = []\n","            all_labels = []\n","\n","            # Wrap test_loader with tqdm for progress bar\n","            val_progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\", leave=False)\n","            with torch.no_grad():\n","                for batch in val_progress_bar:\n","                    input_ids, attention_mask, labels = [b.to(self.device) for b in batch]\n","\n","                    outputs = self.model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels\n","                    )\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(logits, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","                    all_preds.extend(predicted.cpu().numpy())\n","                    all_labels.extend(labels.cpu().numpy())\n","\n","                    # Update progress bar\n","                    val_progress_bar.set_postfix({\n","                        'Loss': f'{loss.item():.4f}',\n","                        'Accuracy': f'{100 * val_correct / val_total:.2f}%'\n","                    })\n","\n","            # Print epoch statistics\n","            # print(f\"Epoch {epoch+1}/{epochs}\")\n","            # print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n","            # print(f\"Train Accuracy: {100 * train_correct / train_total:.2f}%\")\n","            # print(f\"Val Loss: {val_loss/len(test_loader):.4f}\")\n","            # print(f\"Val Accuracy: {100 * val_correct / val_total:.2f}%\")\n","\n","            # Compute detailed classification report\n","            # report = classification_report(all_labels, all_preds)\n","            # print(\"Classification Report:\")\n","            # print(report)\n","\n","        return self.model\n","\n","    def custom_train(self, text_column, label_column, epochs=1, learning_rate=2e-5):\n","        \"\"\"\n","        Custom training method with tqdm progress bars\n","\n","        :param train_df: Training dataframe with tokenized column\n","        :param test_df: Test dataframe with tokenized column\n","        :param label_column: Name of the label column\n","        :param epochs: Number of training epochs\n","        :param learning_rate: Learning rate for optimizer\n","        \"\"\"\n","        # Prepare data tensors\n","        # print(train_df[text_column].shape)\n","        # train_input_ids = train_df[text_column]\n","        # train_attention_mask = torch.tensor([x['attention_mask'] for x in train_df['tokenized']])\n","        # train_labels = torch.tensor(train_df[label_column].values)\n","\n","        # test_input_ids = test_df[text_column]\n","        # test_attention_mask = torch.tensor([x['attention_mask'] for x in test_df['tokenized']])\n","        # test_labels = torch.tensor(test_df[label_column].values)\n","\n","\n","        # train_labels = torch.tensor(np.array(train_df[label_column].to_list(), dtype=np.int64), dtype=torch.long)\n","        # test_labels = torch.tensor(np.array(test_df[label_column].to_list(), dtype=np.int64), dtype=torch.long)\n","\n","        # Create data loaders\n","        # train_dataset = torch.utils.data.TensorDataset(\n","        #     train_input_ids,\n","        #     # train_attention_mask,\n","        #     train_labels\n","        # )\n","        # test_dataset = torch.utils.data.TensorDataset(\n","        #     test_input_ids,\n","        #     # test_attention_mask,\n","        #     test_labels\n","        # )\n","\n","        train_loader = torch.utils.data.DataLoader(\n","            self.train_data,\n","            batch_size=1,\n","            shuffle=True\n","        )\n","        test_loader = torch.utils.data.DataLoader(\n","            self.test_data,\n","            batch_size=1,\n","            shuffle=True\n","        )\n","\n","        # Prepare optimizer and loss\n","        optimizer = optim.AdamW(\n","            self.model.parameters(),\n","            lr=learning_rate\n","        )\n","        criterion = nn.CrossEntropyLoss()\n","\n","        # Training loop with tqdm progress bars\n","        for epoch in range(epochs):\n","            # Training phase\n","            self.model.train()\n","            train_loss = 0\n","            train_correct = 0\n","            train_total = 0\n","\n","            # Wrap train_loader with tqdm for progress bar\n","            train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Training)\", leave=False)\n","            for batch in train_progress_bar:\n","                # Unpack batch\n","                input_ids, labels = [b for b in batch]\n","                # print(len(input_ids))\n","\n","                labels = labels.to(self.device)\n","\n","                # Zero gradients\n","                optimizer.zero_grad()\n","\n","                # Forward pass\n","                outputs = self.model(\n","                    input_ids[0],\n","                    # attention_mask=attention_mask,\n","                    labels=labels[0].to(self.device)\n","                )\n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","                # Backward pass\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Compute accuracy\n","                train_loss += loss.item()\n","                _, predicted = torch.max(logits, 1)\n","                train_total += labels.size(0)\n","                train_correct += (predicted == labels).sum().item()\n","\n","                # Update progress bar\n","                train_progress_bar.set_postfix({\n","                    'Loss': f'{loss.item():.4f}',\n","                    'Accuracy': f'{100 * train_correct / train_total:.2f}%'\n","                })\n","\n","            # Validation phase\n","            self.model.eval()\n","            val_loss = 0\n","            val_correct = 0\n","            val_total = 0\n","            all_preds = []\n","            all_labels = []\n","\n","            # Wrap test_loader with tqdm for progress bar\n","            val_progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\", leave=False)\n","            with torch.no_grad():\n","                for batch in val_progress_bar:\n","                    input_ids, labels = [b for b in batch]\n","\n","\n","                    labels = labels.to(self.device)\n","\n","                    outputs = self.model(input_ids,\n","                        # attention_mask=attention_mask,\n","                        labels=labels[0]\n","                    )\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(logits, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","                    all_preds.extend(predicted.cpu().numpy())\n","                    all_labels.extend(labels.cpu().numpy())\n","\n","                    # Update progress bar\n","                    val_progress_bar.set_postfix({\n","                        'Loss': f'{loss.item():.4f}',\n","                        'Accuracy': f'{100 * val_correct / val_total:.2f}%'\n","                    })\n","\n","            # Print epoch statistics\n","            print(f\"Epoch {epoch+1}/{epochs}\")\n","            print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n","            print(f\"Train Accuracy: {100 * train_correct / train_total:.2f}%\")\n","            print(f\"Val Loss: {val_loss/len(test_loader):.4f}\")\n","            print(f\"Val Accuracy: {100 * val_correct / val_total:.2f}%\")\n","\n","            # Compute detailed classification report\n","            # report = classification_report(all_labels, all_preds)\n","            print(\"Classification Report:\")\n","            # print(report)\n","\n","        return self.model\n","\n","    def predict(self, texts, start=0):\n","        \"\"\"\n","        Make predictions on new texts\n","\n","        :param texts: List of text strings to classify\n","        :return: Predictions and their probabilities\n","        \"\"\"\n","        # Tokenize inputs\n","        inputs = self.tokenizer(\n","            texts[start:],\n","            padding=True,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors='pt'\n","        ).to(self.device)\n","\n","        # Get model predictions\n","        self.model.eval()\n","        with torch.no_grad():\n","            outputs = self.model(**inputs)\n","            probs = torch.softmax(outputs.logits, dim=1)\n","            predictions = torch.argmax(probs, dim=1)\n","\n","        return predictions.cpu().numpy(), probs.cpu().numpy()\n","\n","\n","    def freeze(self):\n","        for name, param in self.model.named_parameters():\n","            if 'classifier' not in name and 'cls_embedding' not in name and 'transformer' not in name:\n","                param.requires_grad = False\n","\n","    def predict_iterate(self, texts):\n","        \"\"\"\n","        Make predictions on new texts\n","\n","        :param texts: List of text strings to classify\n","        :return: Predictions and their probabilities\n","        \"\"\"\n","        # Tokenize inputs\n","        inputs = self.tokenizer(\n","            texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors='pt'\n","        ).to(self.device)\n","\n","        # Get model predictions\n","        self.model.eval()\n","        with torch.no_grad():\n","            outputs = self.model(**inputs)\n","            probs = torch.softmax(outputs.logits, dim=1)\n","            predictions = torch.argmax(probs, dim=1)\n","\n","        return predictions.cpu().numpy(), probs.cpu().numpy()"],"metadata":{"id":"tbOVOycMkkbm","executionInfo":{"status":"ok","timestamp":1734007769483,"user_tz":-60,"elapsed":2214,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Initialize classifier\n","classifier = TweetSentenceClassifier()  # Specify number of labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPR1YTrcs_02","executionInfo":{"status":"ok","timestamp":1734007770858,"user_tz":-60,"elapsed":1377,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"075902c1-5119-439b-b8b3-b77857350b0d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["classifier.freeze()"],"metadata":{"id":"FG_s7Ixd1yYk","executionInfo":{"status":"ok","timestamp":1734007770859,"user_tz":-60,"elapsed":3,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Prepare dataset (adjust column names as needed)\n","classifier.prepare_dataset(\n","    df,\n","    text_column=2,\n","    label_column=1\n",")"],"metadata":{"id":"_T-9RXUsmnrc","executionInfo":{"status":"ok","timestamp":1734007772244,"user_tz":-60,"elapsed":2,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["for name, param in classifier.model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"Trainable parameter: {name}\")\n","    else:\n","        print(f\"Non-trainable parameter: {name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcaUlAEpyMTU","executionInfo":{"status":"ok","timestamp":1734007773980,"user_tz":-60,"elapsed":7,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"8365cd5d-6439-4348-d4ec-5a76e4b2f926"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Non-trainable parameter: distilbert.distilbert.embeddings.word_embeddings.weight\n","Non-trainable parameter: distilbert.distilbert.embeddings.position_embeddings.weight\n","Non-trainable parameter: distilbert.distilbert.embeddings.LayerNorm.weight\n","Non-trainable parameter: distilbert.distilbert.embeddings.LayerNorm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.0.output_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.1.output_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.2.output_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.3.output_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.4.output_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.q_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.q_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.k_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.k_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.v_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.v_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.out_lin.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.attention.out_lin.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.sa_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.sa_layer_norm.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.ffn.lin1.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.ffn.lin1.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.ffn.lin2.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.ffn.lin2.bias\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.output_layer_norm.weight\n","Trainable parameter: distilbert.distilbert.transformer.layer.5.output_layer_norm.bias\n","Trainable parameter: distilbert.pre_classifier.weight\n","Trainable parameter: distilbert.pre_classifier.bias\n","Trainable parameter: distilbert.classifier.weight\n","Trainable parameter: distilbert.classifier.bias\n","Trainable parameter: new_embeds.cls_embedding\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.embeddings.word_embeddings.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.embeddings.position_embeddings.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.embeddings.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.embeddings.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.0.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.1.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.2.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.3.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.4.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.5.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.6.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.7.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.8.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.9.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.10.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.q.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.q.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.k.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.k.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.v.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.v.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.o.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.attn.o.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.attention.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.intermediate.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.intermediate.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.output.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.output.dense.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.output.LayerNorm.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.layer.11.output.LayerNorm.bias\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.encoder.relative_attention_bias.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.pooler.dense.weight\n","Non-trainable parameter: new_embeds.sentence_model.0.auto_model.pooler.dense.bias\n","Trainable parameter: classifier.weight\n","Trainable parameter: classifier.bias\n"]}]},{"cell_type":"code","source":["# Train the model\n","model = classifier.custom_train(2, 1, 2, learning_rate=10e-7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343,"referenced_widgets":["e88a4cab91984054a3c2579460968cbb","fe0e172f2b224564a3799b9f836e0498","3a83e8fa5a3045969c583d6e43e4bfda","5687a8ce93e54e029d9092aac62f38af","2aefab9e4af64738981032fc9a19ba85","5b39d8ab81864c51b8ad6a4591eb2866","ad040509b5fe43f5808580709a7c05d9","a239185ff0c9450483baf19b85844283","3963aa23064e4530b247cc47e23acbbe","79366bea020a460190b60b43d27a3b82","914865dd3b494f05bf5b2460c753e5e5","75087e4687764a808454c6a5557a4ca0","bd1107616d21443e8f308c09c3db4152","32e6631819314646824eded5ce16ddfe","3c7f381552d4469c8e3dddc76803fd85","877e7cb7cd804d8c9e73617b36e710b7","cece020b6a544c64aa436f19f5cc3e36","75c5756f8aee484881f565f33a09f38c","f92db50834e3469bb54414142b2aab2a","4c1ea37f5e0b45e7bceff0a50627cb3d","7cf8ee68bdd54c6e96bbb75063613198","d5b7842737934112989806768283c7f3"]},"id":"E2_0NLqIm1jS","executionInfo":{"status":"error","timestamp":1734009502482,"user_tz":-60,"elapsed":747707,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"c305f052-6847-4e92-fa89-cb64904c907c"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Epoch 1/2 (Training):   0%|          | 0/1709 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88a4cab91984054a3c2579460968cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Epoch 1/2 (Validation):   0%|          | 0/428 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75087e4687764a808454c6a5557a4ca0"}},"metadata":{}},{"output_type":"error","ename":"IndexError","evalue":"tuple index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-f233d3be2bdb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-28bcd5da4301>\u001b[0m in \u001b[0;36mcustom_train\u001b[0;34m(self, text_column, label_column, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                     outputs = self.model(input_ids, \n\u001b[0m\u001b[1;32m    464\u001b[0m                         \u001b[0;31m# attention_mask=attention_mask,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3b4856fb770c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, labels)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Get the custom embeddings from the Sentence-BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# print(\"Embeddings shape:\", embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3b4856fb770c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(sentence_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Concatenate CLS embedding with sentence embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \"\"\"\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 \u001b[0mbatch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mto_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"cell_type":"code","source":["classifier.model(\"awdawd\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"S9DjebQWppiY","executionInfo":{"status":"error","timestamp":1734006733612,"user_tz":-60,"elapsed":553,"user":{"displayName":"Isai Gordeev","userId":"08863145465111743838"}},"outputId":"ac8a282a-7278-4a15-d9bc-95a100d9dfb1"},"execution_count":60,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"CustomDistilBERTModelSimple.forward() missing 1 required positional argument: 'labels'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-188a933a7070>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"awdawd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: CustomDistilBERTModelSimple.forward() missing 1 required positional argument: 'labels'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CfC8P2Ts2vbL"},"execution_count":null,"outputs":[]}]}