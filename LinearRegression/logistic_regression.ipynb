{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../DataPreprocessing/cleaned_train_dataset_megafinal_processed.npy\", allow_pickle=True)\n",
    "y = np.load(\"../DataPreprocessing/cleaned_eval_dataset_training.npy\", allow_pickle=True)\n",
    "\n",
    "df_X = pd.DataFrame(X)\n",
    "df_y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length:  1641\n",
      "Max length:  195007\n"
     ]
    }
   ],
   "source": [
    "# Print min and max length of tweet in symbols\n",
    "print(\"Min length: \", df_X[2].apply(len).min())\n",
    "print(\"Max length: \", df_X[2].apply(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model to generate sentence embeddings\n",
    "embedding_model = api.load(\"glove-twitter-200\")\n",
    "\n",
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "tweet_vectors = np.vstack([get_avg_embedding(tweet, embedding_model, vector_size) for tweet in df_X[2]])\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df_X, tweet_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  0.6386292834890965\n"
     ]
    }
   ],
   "source": [
    "# Обучи модель логистической регрессии на данных. Для обучения на вход нужно подавать данные df_X['embedding'], а на выходе df_X[1]. \n",
    "# Для тестирования d_y[1]. Полученные после тестирования ответы сохрани в d_y['predicted'].\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X['embeddings'].to_list(), df_X[1].to_list(), test_size=0.15, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_predicted = logistic_model.predict(list(X_test))\n",
    "\n",
    "# Save the results\n",
    "df_y['predicted'] = logistic_model.predict(list(df_y['embeddings']))\n",
    "\n",
    "# Calculate the mean squared error\n",
    "print(\"Error \", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "df_X['predicted'] = logistic_model.predict(list(df_X['embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7384183434721572\n"
     ]
    }
   ],
   "source": [
    "# count error acuracy score\n",
    "print(accuracy_score(df_X[1].to_list(), df_X['predicted'].to_list()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 14:07:55,796] A new study created in memory with name: no-name-a95e366c-c5a4-425e-b90e-723a7dcc0b51\n",
      "[I 2024-12-12 14:07:56,370] Trial 0 finished with value: 0.3800623052959502 and parameters: {'C': 8627.5051487697, 'max_iter': 8438}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:07:59,101] Trial 1 finished with value: 0.3800623052959502 and parameters: {'C': 5256.265622360428, 'max_iter': 5666}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:00,476] Trial 2 finished with value: 0.3800623052959502 and parameters: {'C': 6478.790549162912, 'max_iter': 8050}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:01,679] Trial 3 finished with value: 0.3800623052959502 and parameters: {'C': 9286.715529130672, 'max_iter': 8436}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:03,495] Trial 4 finished with value: 0.383177570093458 and parameters: {'C': 1235.9310813422628, 'max_iter': 7466}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:05,904] Trial 5 finished with value: 0.3800623052959502 and parameters: {'C': 6836.244201414109, 'max_iter': 4609}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:08,191] Trial 6 finished with value: 0.383177570093458 and parameters: {'C': 1348.080237724475, 'max_iter': 4143}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:10,206] Trial 7 finished with value: 0.3800623052959502 and parameters: {'C': 9599.068712152099, 'max_iter': 8498}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:12,247] Trial 8 finished with value: 0.383177570093458 and parameters: {'C': 2667.394843591045, 'max_iter': 8404}. Best is trial 0 with value: 0.3800623052959502.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-12-12 14:08:13,177] Trial 9 finished with value: 0.3800623052959502 and parameters: {'C': 5054.931451876866, 'max_iter': 397}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:14,782] Trial 10 finished with value: 0.3800623052959502 and parameters: {'C': 7959.14061332393, 'max_iter': 9930}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:15,984] Trial 11 finished with value: 0.3800623052959502 and parameters: {'C': 4422.138246628058, 'max_iter': 5866}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:17,173] Trial 12 finished with value: 0.3800623052959502 and parameters: {'C': 4233.872691387095, 'max_iter': 2619}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:18,329] Trial 13 finished with value: 0.3800623052959502 and parameters: {'C': 7728.139769922392, 'max_iter': 6450}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:19,475] Trial 14 finished with value: 0.3800623052959502 and parameters: {'C': 5641.035681466231, 'max_iter': 9976}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:20,686] Trial 15 finished with value: 0.3800623052959502 and parameters: {'C': 3335.752328871621, 'max_iter': 3154}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:21,742] Trial 16 finished with value: 0.3800623052959502 and parameters: {'C': 8252.798770802045, 'max_iter': 6401}. Best is trial 0 with value: 0.3800623052959502.\n",
      "[I 2024-12-12 14:08:22,587] Trial 17 finished with value: 0.37071651090342683 and parameters: {'C': 13.341517563237176, 'max_iter': 1606}. Best is trial 17 with value: 0.37071651090342683.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-12-12 14:08:22,894] Trial 18 finished with value: 0.383177570093458 and parameters: {'C': 531.8105948405971, 'max_iter': 161}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:24,082] Trial 19 finished with value: 0.3800623052959502 and parameters: {'C': 2913.181296959619, 'max_iter': 1530}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:25,366] Trial 20 finished with value: 0.3800623052959502 and parameters: {'C': 1827.303951953175, 'max_iter': 3514}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:26,549] Trial 21 finished with value: 0.3800623052959502 and parameters: {'C': 6082.630064984869, 'max_iter': 5375}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:27,742] Trial 22 finished with value: 0.3800623052959502 and parameters: {'C': 8889.791933575087, 'max_iter': 2029}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:28,951] Trial 23 finished with value: 0.3800623052959502 and parameters: {'C': 7236.8795163156165, 'max_iter': 7276}. Best is trial 17 with value: 0.37071651090342683.\n",
      "[I 2024-12-12 14:08:29,800] Trial 24 finished with value: 0.36448598130841126 and parameters: {'C': 17.58281877573898, 'max_iter': 1303}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:30,697] Trial 25 finished with value: 0.3738317757009346 and parameters: {'C': 78.534286551713, 'max_iter': 1133}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:31,897] Trial 26 finished with value: 0.3769470404984424 and parameters: {'C': 305.24610584815156, 'max_iter': 1081}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:32,602] Trial 27 finished with value: 0.36448598130841126 and parameters: {'C': 16.25544533856419, 'max_iter': 1147}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:33,653] Trial 28 finished with value: 0.383177570093458 and parameters: {'C': 2064.957165607397, 'max_iter': 2230}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:34,934] Trial 29 finished with value: 0.383177570093458 and parameters: {'C': 918.9952249384389, 'max_iter': 3113}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:35,832] Trial 30 finished with value: 0.36760124610591904 and parameters: {'C': 35.13544903088883, 'max_iter': 894}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:36,689] Trial 31 finished with value: 0.37071651090342683 and parameters: {'C': 41.444868853581006, 'max_iter': 1131}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:37,846] Trial 32 finished with value: 0.383177570093458 and parameters: {'C': 982.8930304508099, 'max_iter': 636}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:38,986] Trial 33 finished with value: 0.3800623052959502 and parameters: {'C': 1905.0363514537003, 'max_iter': 1773}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:40,164] Trial 34 finished with value: 0.383177570093458 and parameters: {'C': 622.2918048425836, 'max_iter': 2640}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:42,436] Trial 35 finished with value: 0.383177570093458 and parameters: {'C': 1457.1236955979841, 'max_iter': 928}. Best is trial 24 with value: 0.36448598130841126.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-12-12 14:08:43,011] Trial 36 finished with value: 0.383177570093458 and parameters: {'C': 2384.1539058643275, 'max_iter': 100}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:44,000] Trial 37 finished with value: 0.36448598130841126 and parameters: {'C': 17.009042676840558, 'max_iter': 3865}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:45,390] Trial 38 finished with value: 0.383177570093458 and parameters: {'C': 875.5213596753806, 'max_iter': 4322}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:46,554] Trial 39 finished with value: 0.383177570093458 and parameters: {'C': 1270.7468564940793, 'max_iter': 3894}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:47,603] Trial 40 finished with value: 0.3800623052959502 and parameters: {'C': 3219.890506416063, 'max_iter': 2387}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:48,566] Trial 41 finished with value: 0.37071651090342683 and parameters: {'C': 96.52996348116292, 'max_iter': 1615}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:49,644] Trial 42 finished with value: 0.3800623052959502 and parameters: {'C': 435.8606223364794, 'max_iter': 827}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:50,696] Trial 43 finished with value: 0.383177570093458 and parameters: {'C': 1576.5663042878357, 'max_iter': 4917}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:51,694] Trial 44 finished with value: 0.383177570093458 and parameters: {'C': 590.7915161696983, 'max_iter': 1410}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:52,366] Trial 45 finished with value: 0.37071651090342683 and parameters: {'C': 7.194030303918154, 'max_iter': 3053}. Best is trial 24 with value: 0.36448598130841126.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-12-12 14:08:53,217] Trial 46 finished with value: 0.383177570093458 and parameters: {'C': 1049.9965021306054, 'max_iter': 452}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:54,512] Trial 47 finished with value: 0.3800623052959502 and parameters: {'C': 2370.8041972140204, 'max_iter': 1770}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:56,453] Trial 48 finished with value: 0.383177570093458 and parameters: {'C': 643.2442096623121, 'max_iter': 2252}. Best is trial 24 with value: 0.36448598130841126.\n",
      "[I 2024-12-12 14:08:58,006] Trial 49 finished with value: 0.3800623052959502 and parameters: {'C': 4076.794970884548, 'max_iter': 3486}. Best is trial 24 with value: 0.36448598130841126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'C': 17.58281877573898, 'max_iter': 1303}\n",
      "Лучший MSE: 0.6355\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Гиперпараметры для оптимизации\n",
    "    C = trial.suggest_float(\"C\", 1e-4, 1e4)  # Регуляризация\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 10000)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = LogisticRegression(C=C, max_iter=max_iter, solver=\"lbfgs\", random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    y_pred = model.predict(X_test)\n",
    "    return 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Запуск оптимизации\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Результаты оптимизации\n",
    "best_params = study.best_params\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "\n",
    "# Обучение с оптимальными гиперпараметрами\n",
    "best_model = LogisticRegression(**best_params, solver=\"lbfgs\", random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка производительности\n",
    "y_predicted = best_model.predict(X_test)\n",
    "mse = accuracy_score(y_test, y_predicted)\n",
    "print(f\"Лучший MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_y \u001b[38;5;241m=\u001b[39m \u001b[43mdf_y\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      2\u001b[0m df_y \u001b[38;5;241m=\u001b[39m df_y\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_y' is not defined"
     ]
    }
   ],
   "source": [
    "df_y = df_y.rename(columns={0: 'ID'})\n",
    "df_y = df_y.rename(columns={'predicted': 'EventType'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохрани айди и ивент тайп в csv\n",
    "df_y[['ID', 'EventType']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted\n",
       "1    277\n",
       "0    239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посчитай кол-во нулей и единиц в колонке predicted df_y\n",
    "df_y['predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_new = pd.DataFrame(X)\n",
    "df_y_new = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переименуй в df_X_new 0 в ID, 1 в EventType, 2 в Tweet\n",
    "df_X_new = df_X_new.rename(columns={0: 'ID', 1: 'EventType', 2: 'Tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_new = df_y_new.rename(columns={0: 'ID', 1: 'Tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df_X_new['Tweet']])\n",
    "tweet_df = pd.DataFrame(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df_X_new, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Tweet'])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['ID']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'ID'])\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.7679127725856698\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf_ = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 100).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.7679127725856698\n"
     ]
    }
   ],
   "source": [
    "y_pred_ = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df_y_new['Tweet']])\n",
    "tweet_df = pd.DataFrame(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df_y_new, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Tweet'])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['ID']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = period_features.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add y_pred as a column to dataframe df_y_new\n",
    "df_y_new['EventType'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventType\n",
       "1    309\n",
       "0    207\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_new['EventType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохрани айди и ивент тайп в csv\n",
    "df_y[['ID', 'EventType']].to_csv('submission_tweet_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
