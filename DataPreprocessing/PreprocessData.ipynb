{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYOE5ZblBozK",
    "outputId": "55b7cf15-27f6-4d2f-f937-450d80e28532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alena_khg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alena_khg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Zm0yPngnFB1v"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing punctuation, numbers, stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JXCUZc5CODsc"
   },
   "outputs": [],
   "source": [
    "def format_number_to_string(number):\n",
    "    \"\"\"\n",
    "    Formats a number into a 3-character string, adding leading zeros if necessary.\n",
    "\n",
    "    Args:\n",
    "        number (int): A number between 0 and 999.\n",
    "\n",
    "    Returns:\n",
    "        str: A string of length 3.\n",
    "    \"\"\"\n",
    "    if not (0 <= number <= 999):\n",
    "        raise ValueError(\"The number must be in the range 0 to 999.\")\n",
    "\n",
    "    return f\"{number:03d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HJ8iluEWBrzD"
   },
   "outputs": [],
   "source": [
    "def process_csv(file_path, l, with_period_id, with_event_type):\n",
    "    \"\"\"\n",
    "    Process a CSV file to extract and tokenize data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        l (int): Desired length of token arrays for the 'Tweet' column.\n",
    "        with_period_id (bool): Whether to include the 'PeriodID' in the tweet text.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with columns 'PeriodID', 'EventType', and 'Tweet'.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer (default tokenizer from transformers)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract required columns\n",
    "    if with_event_type:\n",
    "        df = df[['PeriodID', 'EventType', 'Tweet']]\n",
    "    else:\n",
    "        df = df[['PeriodID', 'Tweet']]\n",
    "\n",
    "    if with_period_id:\n",
    "      # Preprocess text and concatenate with formatted PeriodID\n",
    "        df['Tweet'] = df.apply(\n",
    "            lambda row: f\"{format_number_to_string(row['PeriodID'])} {preprocess_text(row['Tweet'])}\",\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "      # Apply preprocessing to each tweet\n",
    "      df['Tweet'] = df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    # Tokenize the 'Tweet' column and pad/truncate to length l\n",
    "    def tokenize_tweet(tweet):\n",
    "        tokens = tokenizer.encode(tweet, truncation=True, padding=\"max_length\", max_length=l, add_special_tokens=True)\n",
    "        return tokens\n",
    "\n",
    "    df['Tweet'] = df['Tweet'].apply(tokenize_tweet)\n",
    "\n",
    "    if with_event_type:\n",
    "        df = df[['EventType', 'Tweet']]\n",
    "    else:\n",
    "        df = df[['Tweet']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of reading the csv file and return the processed data\n",
    "def read_csv(folder_path, with_period_id, with_event_type, l=128):\n",
    "    \"\"\"\n",
    "    Read all CSV files in a folder and process them.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the CSV files.\n",
    "        l (int): Desired length of token arrays for the 'Tweet' column.\n",
    "        with_period_id (bool): Whether to include the 'PeriodID' in the tweet text.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with columns 'PeriodID', 'EventType' (optional), and 'Tweet' (tokenized).\n",
    "    \"\"\"\n",
    "    li = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        df = process_csv(folder_path + filename, l, with_period_id, with_event_type)\n",
    "        li.append(df)\n",
    "    df = pd.concat(li, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "vSAA7er1CE4t",
    "outputId": "05e22f93-5bf6-4ec5-e508-017c4c224169"
   },
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe with Period Id in tweet tokens\n",
    "df_train_with_period_id = read_csv(\"../train_tweets/\", True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_without_period_id = read_csv(\"../train_tweets/\", False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_with_period_id = read_csv(\"../eval_tweets/\", True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_without_period_id = read_csv(\"../eval_tweets/\", False, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
